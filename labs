# -*- coding: utf-8 -*-
"""CS6319Cluster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VnR0jJDf5kyWUDDVBEGCPJLOxnz0ndi1

Here's what we did in the lecture notes.  Let's experiments with KMeans, first by fitting it to some points.
"""

import numpy as np
import pandas as pd
import sklearn
from sklearn.cluster import KMeans

X = np.array([[1,2],[1,4],[1,0],[10,2],[10,4],[10,0]])
km = KMeans(n_clusters=2).fit(X)
print(km.labels_)

"""We can now print out the centroids of the clusters, which are called cluster_centers_ in sklearn."""

print(km.cluster_centers_)

"""Now predict the labels of some new data points."""

labs = km.predict([[0, 0], [12, 3]])
print(labs)

"""The quality of a clustering can be measured by its inertia."""

print(km.inertia_)

"""Or by its score."""

print(km.score(X))

"""If we know what the labels should be (the "ground truth") then we can measure the accuracy of our predictions using rand_score.  We need this because the names of the labels might be permuted.  Suppose the correct labels are [0,0,0,1,1,1,2,2,2] but k-means predicts [2,2,2,0,0,0,1,1,1]: this is also correct as it clusters points in the same way, but it just permuted the cluster names.  rand_score can handle this:"""

from sklearn import metrics

print(metrics.rand_score([0,0,0,1,1,1,2,2,2],[2,2,2,0,0,0,1,1,1]))

"""What value should K (n_clusters) be?  If we set it to different values and plot score or inertia, we often see an "elbow" in the curve.  But a better way is to find the silhouette score of the clustering for different K, and the best score tells us what K should be."""

from sklearn.metrics import silhouette_score

s = silhouette_score(X, km.labels_)
print(s)

"""Now instead of K-means let's use expectation maximisation (EM) for clustering, by fitting a Gaussian mixture model (GMM).  First generate a dataset:"""

from numpy import hstack
from numpy.random import normal
from matplotlib import pyplot

X1 = normal(loc=20, scale=5, size=3000)
X2 = normal(loc=40, scale=5, size=7000)
X = hstack((X1, X2))
X = X.reshape((len(X), 1))

"""A histogram shows the 2 clusters, which overlap."""

pyplot.hist(X, bins=50)
pyplot.show()

"""Now try to recreate the X1 and X2 data by fitting a GMM."""

from sklearn.mixture import GaussianMixture

gm = GaussianMixture(n_components=2)
gm.fit(X)

"""Now we can predict labels y for any points X.  To illustrate this, let's do it for the training data."""

y = gm.predict(X)
print(y[:100])
print(y[-100:])

"""We can also use a GMM *generatively*, to generate new data from the same distribution."""

Xnew, ynew = gm.sample(6)
print(Xnew)
print(ynew)

"""Let's also see DBSCAN in action.  Notice that we don't need to tell it K, but it does have other parameters such as eps(ilon) and min_samples."""

from sklearn.cluster import DBSCAN

X = np.array([[1,2],[2,2],[2,3],[8,7],[8,8],[25,80]])
db = DBSCAN(eps=3, min_samples=2)
clustering = db.fit(X)
print(clustering.labels_)

"""The last point (with label -1) is an outlier that doesn't seem to belong to any cluster."""





# -*- coding: utf-8 -*-
"""CS6319ARM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kTs6bCOan7tDJmM8LJgCQk5TzHQsEnAz

Use apyori.
"""

!pip install apyori

import numpy as np
import pandas as pd
from apyori import apriori

"""Read file."""

basket = pd.read_csv("https://github.com/andvise/DataAnalyticsDatasets/blob/78a9bd799b5fbf35344beff50304169f789d264c/Market_Basket_Optimisation.csv?raw=true",header=None)
print(basket)

"""Transform to a list of lists."""

records = []
for i in range(0, 7501):
    records.append([str(basket.values[i,j]) for j in range(0, 20)])

"""Apply Apriori."""

rules = apriori(records, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=2)
results = list(rules)

"""Count the rules."""

print(len(results))

"""View results."""

print(results[0])

"""All results."""

for item in results:
    pair = item[0]
    items = [x for x in pair]
    print("Rule: " + items[0] + " -> " + items[1])
    print("Support: " + str(item[1]))
    print("Confidence: " + str(item[2][0][2]))
    print("Lift: " + str(item[2][0][3]))
    print("=====================================")



# -*- coding: utf-8 -*-
"""CS6319DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rBuM7lYPEXGmCaNOc7_V9tjmxYiEiw7i

MLPRegressor experiment on Parkinsons data.
"""

import pandas as pd
import numpy as np
import sklearn
park = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data")
print(park)

"""Drop subject and motor_UPDRS columns."""

park.drop(labels=['subject#','motor_UPDRS'], axis=1, inplace=True)

"""Split into train and test."""

from sklearn.model_selection import train_test_split

parktrain, parktest = train_test_split(park, test_size=0.2)

"""Extract labels."""

trainlabs = parktrain.loc[:,'total_UPDRS']
traindata = parktrain.drop(labels=['total_UPDRS'], axis=1)
testlabs = parktest.loc[:,'total_UPDRS']
testdata = parktest.drop(labels=['total_UPDRS'], axis=1)

"""Rescale."""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
traindatar = scaler.fit_transform(traindata)
testdatar = scaler.transform(testdata)

"""Try a linear regressor on the training data with cross-validation."""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit
from sklearn.linear_model import LinearRegression

linreg = LinearRegression()
split = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(linreg, traindatar, trainlabs, scoring="neg_mean_squared_error", cv=split)
rmse = np.sqrt(-scores)
print("LS RMSE =",rmse.mean()," stddev =",scores.std())

"""And a random forest."""

from sklearn.ensemble import RandomForestRegressor

rfreg = RandomForestRegressor()
scores = cross_val_score(rfreg, traindatar, trainlabs, scoring="neg_mean_squared_error", cv=split)
rmse = np.sqrt(-scores)
print("RF RMSE =",rmse.mean()," stddev =",scores.std())

"""Better!  Now an MLP."""

from sklearn.neural_network import MLPRegressor

mlpreg = MLPRegressor(max_iter=10000)
scores = cross_val_score(mlpreg, traindatar, trainlabs, scoring="neg_mean_squared_error", cv=split)
rmse = np.sqrt(-scores)
print("MLP RMSE =",rmse.mean()," stddev =",scores.std())

"""Similar to RF.  Can we improve it?  """

mlpreg = MLPRegressor(hidden_layer_sizes=(30,20,10), max_iter=10000)
scores = cross_val_score(mlpreg, traindatar, trainlabs, scoring="neg_mean_squared_error", cv=split)
rmse = np.sqrt(-scores)
print("MLP RMSE =",rmse.mean()," stddev =",scores.std())

"""A bit better but not as good as RF.  Now try on the test data."""

from sklearn.metrics import mean_squared_error

linreg.fit(traindatar,trainlabs)
y = linreg.predict(testdatar)
mse = mean_squared_error(testlabs,y)
rmse = np.sqrt(mse)
print("LIN test error = ", rmse)

rfreg.fit(traindatar,trainlabs)
y = rfreg.predict(testdatar)
mse = mean_squared_error(testlabs,y)
rmse = np.sqrt(mse)
print("RF test error = ", rmse)

mlpreg.fit(traindatar,trainlabs)
y = mlpreg.predict(testdatar)
mse = mean_squared_error(testlabs,y)
rmse = np.sqrt(mse)
print("MLP test error = ", rmse)

"""Similar to the training data results: MLP similar to linear regressor, but RF much better.

Try MLPClassifier on the electrical grid stability dataset.  First the data wrangling as before.
"""

import pandas as pd
import numpy as np
import sklearn

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPClassifier

griddata = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv")

griddata = griddata.drop("stab", axis=1)
gridvars = griddata.drop("stabf", axis=1)
gridlabs = griddata["stabf"].copy()

scaler = MinMaxScaler()
gridvarsr = scaler.fit_transform(gridvars)

le = LabelEncoder()
gridlabse = le.fit_transform(gridlabs)

"""Now the classifier."""

classifier = MLPClassifier(hidden_layer_sizes=(200), max_iter=1000)
split = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(classifier, gridvarsr, gridlabse, scoring="accuracy", cv=split)
print("MLP accuracy =",scores.mean()," stddev =",scores.std())

"""Now let's see Keras and TensorFlow, using the grid classification problem.  Import libraries and wrangle the data as above, but this time I'll split it into train and test."""

import pandas as pd
import numpy as np
import sklearn
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

griddata = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv")

griddata = griddata.drop("stab", axis=1)
gridtrain, gridtest = train_test_split(griddata, test_size=0.2)

traindata = gridtrain.drop("stabf", axis=1)
trainlabs = gridtrain["stabf"].copy()
testdata = gridtest.drop("stabf", axis=1)
testlabs = gridtest["stabf"].copy()

scaler = MinMaxScaler()
traindatar = scaler.fit_transform(traindata)
testdatar = scaler.transform(testdata)

le = LabelEncoder()
trainlabse = le.fit_transform(trainlabs)
testlabse = le.transform(testlabs)

"""Now build an ANN sequentially in Keras."""

model = keras.models.Sequential()
model.add(keras.layers.Dense(200, activation="relu"))
model.add(keras.layers.Dense(1, activation="sigmoid"))
model.compile(loss="binary_crossentropy", metrics=["accuracy"])
model.fit(traindatar, trainlabse, epochs=50)
model.evaluate(testdatar, testlabse)

"""Now MNIST as an example of multiclass classification with Keras."""

import pandas as pd
import numpy as np
import sklearn
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import OneHotEncoder

mnisttrain = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra", header=None)
mnisttest = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes", header=None)

traindata = mnisttrain.drop(64, axis=1)
trainlabs = mnisttrain[64].copy()
testdata = mnisttest.drop(64, axis=1)
testlabs = mnisttest[64].copy()
print(traindata)
enc = OneHotEncoder(sparse=False)
trainlabs = enc.fit_transform(trainlabs.to_numpy().reshape(-1, 1))
testlabs = enc.transform(testlabs.to_numpy().reshape(-1, 1))

"""Fit the network."""

model = keras.models.Sequential()
model.add(keras.layers.Dense(50, activation="relu", input_shape=(64,)))
model.add(keras.layers.Dense(10, activation="softmax"))
model.compile(loss="categorical_crossentropy", metrics=["accuracy"])
model.fit(traindata, trainlabs, epochs=50)

"""Finally, evaluate it on the test data."""

model.evaluate(testdata, testlabs)

"""Good result, but there seems to be a bit of overfitting.  Let's  use dropout.  I'll also add another hidden layer to show the use of dropout better."""

from keras.layers import Dropout

model = keras.models.Sequential()
model.add(Dropout(0.1, input_shape=(64,)))
model.add(keras.layers.Dense(50, activation="relu"))
model.add(Dropout(0.1))
model.add(keras.layers.Dense(50, activation="relu"))
model.add(keras.layers.Dense(10, activation="softmax"))
model.compile(loss="categorical_crossentropy", metrics=["accuracy"])
model.fit(traindata, trainlabs, epochs=50)

"""Evaluate again."""

model.evaluate(testdata, testlabs)

"""Nice: the train and test accuracies are about the same.

Now let's apply Keras to regression on the housing data.  For simplicity let's just use the numerical attributes (including the 3 new ones we created).
"""

import numpy as np
import pandas as pd
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras import layers
from keras.layers import Dropout

housing = pd.read_csv("https://github.com/ageron/handson-ml/raw/master/datasets/housing/housing.csv")

trainset, testset = train_test_split(housing, test_size=0.2)

traindata = trainset.drop("median_house_value", axis=1)
trainlabs = trainset["median_house_value"].copy()
trainnum = traindata.drop("ocean_proximity", axis=1)
testdata = testset.drop("median_house_value", axis=1)
testlabs = testset["median_house_value"].copy()
testnum = testdata.drop("ocean_proximity", axis=1)

trainnum["rooms_per_household"] = trainnum["total_rooms"] / trainnum["households"]
trainnum["bedrooms_per_room"] = trainnum["total_bedrooms"] / trainnum["total_rooms"]
trainnum["population_per_household"] = trainnum["population"] / trainnum["households"]
testnum["rooms_per_household"] = testnum["total_rooms"] / testnum["households"]
testnum["bedrooms_per_room"] = testnum["total_bedrooms"] / testnum["total_rooms"]
testnum["population_per_household"] = testnum["population"] / testnum["households"]

imputer = SimpleImputer(strategy="median")
imputer.fit(trainnum)
X = imputer.transform(trainnum)
trainnum = pd.DataFrame(X, columns=trainnum.columns)
Y = imputer.transform(testnum)
testnum = pd.DataFrame(Y, columns=testnum.columns)

scaler = StandardScaler()
trainnumr = scaler.fit_transform(trainnum)
testnumr = scaler.transform(testnum)

"""Fit a network to the training data."""

model = keras.models.Sequential()
model.add(keras.layers.Dense(100, activation="relu", input_shape=(11,)))
model.add(keras.layers.Dense(100, activation="relu"))
model.add(keras.layers.Dense(100, activation="relu"))
model.add(keras.layers.Dense(1))
model.compile(loss="mean_squared_error")
model.fit(trainnumr, trainlabs, epochs=300)

"""Evaluate on the test data."""

model.evaluate(testnumr, testlabs)

"""Pretty good: beats all other classifiers we tried, with little sign of overfitting even without dropout."""


# -*- coding: utf-8 -*-
"""CS6319Dimensionality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12nXe1MU1fu5HC6Y5JfzM2igEUXCjxBb0

First a data file with a few hundred features.
"""

import pandas as pd
import numpy as np
import sklearn

secomdata = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data",
                        header=None,delim_whitespace=True)
secomlabs = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data",
                        header=None,delim_whitespace=True)
print(secomdata)
print(secomlabs)

"""For this experiment let's not split into train and test: we'll just apply cross-validation on the whole dataset.  First discard the 2nd column in the labels DataFrame, and encode the labels as they're in the form -1,+1."""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
secomlabs1 = secomlabs.iloc[:,0]
secomlabs2 = le.fit_transform(secomlabs1)

"""There seem to be missing values so impute the data."""

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")
secomdata = imputer.fit_transform(secomdata)

"""The value ranges are different so rescale.  I'll use normalisation for no particular reason."""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
secomdata = scaler.fit_transform(secomdata)

"""For a change let's try a logistic regression classifier.  It fails to converge with the default number of iterations (100) so I increased it to 400."""

from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(max_iter=400)
split = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(classifier, secomdata, secomlabs2,
                        scoring="accuracy", cv=split)
print("LR acc mean =",scores.mean()," stddev =",scores.std())

"""Now let's try some feature selection methods.  First variance threshold.  Using 0.8 gave an error message (none met the threshold) but 0.9 reduces the number of variables to just 4."""

from sklearn.feature_selection import VarianceThreshold

sel = VarianceThreshold(threshold=(.9 * (1 - .9)))
secomsel = sel.fit_transform(secomdata)
print(secomsel)
split = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(classifier, secomsel, secomlabs2,
                        scoring="accuracy", cv=split)
print("LR acc mean =",scores.mean()," stddev =",scores.std())

"""No real difference in accuracy, and a faster runtime.  (It also works with the default max_iter=100.)  If we tested it on new data we might even find a better result, as overfitting is less likely  with fewer attributes.

Let's try again with a random forest, first on the reduced set of features.
"""

from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier()
scores = cross_val_score(classifier, secomsel, secomlabs2,
                        scoring="accuracy", cv=split)
print("RF acc mean =",scores.mean()," stddev =",scores.std())

"""Very similar result.  The random forest is slower.

Now let's try it on the full set of features.
"""

scores = cross_val_score(classifier, secomdata, secomlabs2,
                        scoring="accuracy", cv=split)
print("RF acc mean =",scores.mean()," stddev =",scores.std())

"""Similar accuracy but a much longer runtime.

Now let's try a higher-dimensional dataset.
"""

import numpy as np
import pandas as pd
import sklearn

arcenedata = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.data",
                        header=None, delim_whitespace=True)
arcenelabs = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.labels",
                        header=None, delim_whitespace=True)
print(arcenedata)
print(arcenelabs.value_counts())

"""Rescale data."""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
arcenedata = scaler.fit_transform(arcenedata)

"""Encode labels."""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
arcenelabs2 = le.fit_transform(arcenelabs.values.ravel())

"""Select features using VarianceThreshold with 0.85 as 0.8 passed no features."""

from sklearn.feature_selection import VarianceThreshold

sel = VarianceThreshold(threshold=(.85 * (1 - .85)))
arcenesel = sel.fit_transform(arcenedata)
print(arcenesel.shape)

"""Classify on all features."""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import ShuffleSplit

split = ShuffleSplit(n_splits=10, test_size=0.2)
classifier = RandomForestClassifier()
scores = cross_val_score(classifier, arcenedata, arcenelabs2,
                        scoring="accuracy", cv=split)
print("acc mean =",scores.mean()," stddev =",scores.std())

"""And on the selected features."""

scores = cross_val_score(classifier, arcenesel, arcenelabs2,
                        scoring="accuracy", cv=split)
print("acc mean =",scores.mean()," stddev =",scores.std())

"""Try Chi2."""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

sel = SelectKBest(chi2, k=100)
arcenesel = sel.fit_transform(arcenedata, arcenelabs2)

"""And classify."""

scores = cross_val_score(classifier, arcenesel, arcenelabs2,
                        scoring="accuracy", cv=split)
print("acc mean =",scores.mean()," stddev =",scores.std())

"""Now try PCA."""

from sklearn.decomposition import PCA

pca = PCA(n_components=30)
arcenepca = pca.fit_transform(arcenedata)

"""And classify."""

scores = cross_val_score(classifier, arcenepca, arcenelabs2,
                        scoring="accuracy", cv=split)
print("acc mean =",scores.mean()," stddev =",scores.std())

"""Let's try a new PCA experiment using one of the datasets provided by sklearn.  We have to transform it to a DataFrame."""

from sklearn.datasets import load_breast_cancer

cancer0 = load_breast_cancer()
cancer = pd.DataFrame(data=cancer0.data, columns=cancer0.feature_names)

"""Take a look at the head of the data."""

cancer.head()

"""And its shape."""

cancer.shape

"""They need rescaling, and I'll use standardisation."""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X = scaler.fit_transform(cancer)

"""Now let's apply PCA.  We don't know how many dimensions to choose, so first let's choose all 30 (which is the default if we don't set n_components)."""

from sklearn.decomposition import PCA

pca30 = PCA()
X30 = pca30.fit_transform(X)

"""Using all features should explain all the variance.  Check this."""

print("explained variance: ",sum(pca30.explained_variance_ratio_))

"""Let's take a look at the explained_variance_ratio_ values using a cumulative sum."""

pca30.explained_variance_ratio_

"""They're listed biggest-first, and we see that the 1st component contains 44% of the variance, etc.  It's more helpful to see a cumulative sum."""

np.cumsum(pca30.explained_variance_ratio_)

"""Now let's see that as a graph."""

import matplotlib.pyplot as plt

plt.plot(np.cumsum(pca30.explained_variance_ratio_))

"""We can see that about 95% of the variance is explained by the first 10 components."""

np.cumsum(pca30.explained_variance_ratio_)[9]

"""Using the first 2 only explains about 63% but that's still useful for visualisation.  Let's plot the 2 components using cancer0.target as colour.  The 2 target values almost separate into 2 clusters."""

pca2 = PCA(n_components=2)
X2 = pca2.fit_transform(X)
plt.scatter(X2[:,0], X2[:,1], c=cancer0.target)



# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13aDenz2TxyxajhtGL8gdvU3ninhfI6UC
"""

import pandas as pd
from itertools import combinations

# a) Read the CSV file into a pandas DataFrame
# Assuming the file has no header and is space-separated
df = pd.read_csv("http://fimi.uantwerpen.be/data/chess.dat", header=None, sep=" ")

# b) Transform the DataFrame into a list of transactions
transactions = df.apply(lambda row: set(row.dropna().astype(str)), axis=1).tolist()

# c) Generate all association rules with length 2, support at least 0.01, confidence at least 2, and lift at least 3
min_support = 0.01
min_confidence = 2
min_lift = 3
rules = []
itemsets = set.union(*transactions)

for item1, item2 in combinations(itemsets, 2):
    support_item1 = sum(1 for transaction in transactions if item1 in transaction) / len(transactions)
    support_item2 = sum(1 for transaction in transactions if item2 in transaction) / len(transactions)
    support_itemset = sum(1 for transaction in transactions if {item1, item2}.issubset(transaction)) / len(transactions)

    print(f"Support for {item1}: {support_item1}")
    print(f"Support for {item2}: {support_item2}")
    print(f"Support for {item1} and {item2}: {support_itemset}")

    if support_itemset >= min_support:
        confidence_item1_to_item2 = support_itemset / support_item1
        confidence_item2_to_item1 = support_itemset / support_item2
        lift_item1_to_item2 = confidence_item1_to_item2 / support_item2
        lift_item2_to_item1 = confidence_item2_to_item1 / support_item1

        print(f"Confidence from {item1} to {item2}: {confidence_item1_to_item2}")
        print(f"Confidence from {item2} to {item1}: {confidence_item2_to_item1}")
        print(f"Lift from {item1} to {item2}: {lift_item1_to_item2}")
        print(f"Lift from {item2} to {item1}: {lift_item2_to_item1}")

        if (confidence_item1_to_item2 >= min_confidence and lift_item1_to_item2 >= min_lift):
            print(f"Rule added: {item1} -> {item2}")
            rules.append((item1, item2, support_itemset, confidence_item1_to_item2, lift_item1_to_item2))
        if (confidence_item2_to_item1 >= min_confidence and lift_item2_to_item1 >= min_lift):
            print(f"Rule added: {item2} -> {item1}")
            rules.append((item2, item1, support_itemset, confidence_item2_to_item1, lift_item2_to_item1))

# d) Print out the rules in a readable form
for rule in rules:
    antecedent = rule[0]
    consequent = rule[1]
    support = round(rule[2], 4)
    confidence = round(rule[3], 4)
    lift = round(rule[4], 4)
    print(f"Rule: {antecedent} -> {consequent} | Support: {support}, Confidence: {confidence}, Lift: {lift}")

# Print the number of rules found
print(f"Number of rules: {len(rules)}")

import pandas as pd
from itertools import combinations

# a) Read the CSV file into a pandas DataFrame
# Assuming the file has no header and is space-separated
df = pd.read_csv("http://fimi.uantwerpen.be/data/chess.dat", header=None, sep=" ")

# b) Transform the DataFrame into a list of transactions
transactions = df.apply(lambda row: set(row.dropna().astype(str)), axis=1).tolist()

# c) Generate all association rules with length 2, support at least 0.01
min_support = 0.01
rules = []
itemsets = set.union(*transactions)

for item1, item2 in combinations(itemsets, 2):
    support_item1 = sum(1 for transaction in transactions if item1 in transaction) / len(transactions)
    support_item2 = sum(1 for transaction in transactions if item2 in transaction) / len(transactions)
    support_itemset = sum(1 for transaction in transactions if {item1, item2}.issubset(transaction)) / len(transactions)

    print(f"Support for {item1}: {support_item1}")
    print(f"Support for {item2}: {support_item2}")
    print(f"Support for {item1} and {item2}: {support_itemset}")

    if support_itemset >= min_support:
        print(f"Rule added: {item1} -> {item2}")
        rules.append((item1, item2, support_itemset))

# d) Print out the rules in a readable form
for rule in rules:
    antecedent = rule[0]
    consequent = rule[1]
    support = round(rule[2], 4)
    print(f"Rule: {antecedent} -> {consequent} | Support: {support}")

# Print the number of rules found
print(f"Number of rules: {len(rules)}")

import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# a) Read the CSV file into a pandas DataFrame
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv"
df = pd.read_csv(url)

# b) Extract the 12 numerical attributes
numerical_attributes = df.select_dtypes(include=['number']).iloc[:, :12]

# c) Apply PCA to find the two main components
pca = PCA(n_components=2)
principal_components = pca.fit_transform(numerical_attributes)

# d) Plot the two components using a scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(principal_components[:, 0], principal_components[:, 1])
plt.title('PCA of 12 Numerical Attributes')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.show()

# e) Find the total explained variance ratio of the two components
total_explained_variance_ratio = sum(pca.explained_variance_ratio_)
print("Total Explained Variance Ratio:", total_explained_variance_ratio)

# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KI6SnJfuf3YV3OmfxEXKpXpIYQYCH9te
"""

import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.metrics import adjusted_rand_score

# (a) Generate artificial 1-dimensional data
data1 = np.random.normal(loc=10, scale=3, size=100)
data2 = np.random.normal(loc=20, scale=2, size=100)
data = np.concatenate([data1, data2])

# (b) Fit a Gaussian mixture model
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(data.reshape(-1, 1))

# (c) Use the “predict” method to obtain predicted cluster labels
predicted_labels = gmm.predict(data.reshape(-1, 1))

# (d) Calculate the Rand score
true_labels = np.concatenate([np.zeros(100), np.ones(100)])  # Assuming 0 for the first 100 data points and 1 for the next 100
rand_score = adjusted_rand_score(true_labels, predicted_labels)
print("Adjusted Rand Score:", rand_score)

import pandas as pd
# Install apyori
!pip install apyori
from apyori import apriori

# (a) Read the CSV file into a pandas DataFrame
url = "http://fimi.uantwerpen.be/data/chess.dat"
df = pd.read_csv(url, header=None, sep=" ")

# (b) Transform the DataFrame into a form that the apyori system can use
transactions = df.apply(lambda row: row.dropna().astype(str).tolist(), axis=1).tolist()

# (c) Generate all association rules with length 2, support at least 0.01, confidence at least 2, and lift at least 3
rules = list(apriori(transactions, min_support=0.01, min_confidence=2, min_lift=3, min_length=2, max_length=2))
print(rules)
# (d) Print out the rules in a readable form
for rule in rules:
    antecedent = ', '.join(rule.items_base)
    consequent = ', '.join(rule.items_add)
    support = round(rule.support, 4)
    confidence = round(rule.ordered_statistics[0].confidence, 4)
    lift = round(rule.ordered_statistics[0].lift, 4)
    print(f"Rule: {antecedent} -> {consequent} | Support: {support}, Confidence: {confidence}, Lift: {lift}")


# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OsrmBsSd6t4DEiYitNDXGIrks6eTsx34
"""

import numpy as np

# Generate data with mean 10 and standard deviation 3
data1 = np.random.normal(loc=10, scale=3, size=100)

# Generate data with mean 20 and standard deviation 2
data2 = np.random.normal(loc=20, scale=2, size=100)

# Combine the two datasets
data = np.concatenate([data1, data2])

# Shuffle the data
np.random.shuffle(data)

# Print the first 10 elements to verify
print(data[:10])

from sklearn.mixture import GaussianMixture

# Create the Gaussian Mixture Model
gmm = GaussianMixture(n_components=2, random_state=42)

# Fit the model to the data
gmm.fit(data.reshape(-1, 1))

# Predict the cluster labels for the data
labels = gmm.predict(data.reshape(-1, 1))

# Print the first 10 labels to verify
print(labels[:10])

from sklearn.metrics import adjusted_rand_score

# Generate true labels based on means
true_labels = np.concatenate([np.zeros(100), np.ones(100)])

# Calculate the adjusted Rand score
rand_score = adjusted_rand_score(true_labels, labels)

# Print the Rand score
print("Adjusted Rand Score:", rand_score)
