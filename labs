# -*- coding: utf-8 -*-
"""CS6319Cluster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VnR0jJDf5kyWUDDVBEGCPJLOxnz0ndi1

Here's what we did in the lecture notes.  Let's experiments with KMeans, first by fitting it to some points.
"""

import numpy as np
import pandas as pd
import sklearn
from sklearn.cluster import KMeans

X = np.array([[1,2],[1,4],[1,0],[10,2],[10,4],[10,0]])
km = KMeans(n_clusters=2).fit(X)
print(km.labels_)

"""We can now print out the centroids of the clusters, which are called cluster_centers_ in sklearn."""

print(km.cluster_centers_)

"""Now predict the labels of some new data points."""

labs = km.predict([[0, 0], [12, 3]])
print(labs)

"""The quality of a clustering can be measured by its inertia."""

print(km.inertia_)

"""Or by its score."""

print(km.score(X))

"""If we know what the labels should be (the "ground truth") then we can measure the accuracy of our predictions using rand_score.  We need this because the names of the labels might be permuted.  Suppose the correct labels are [0,0,0,1,1,1,2,2,2] but k-means predicts [2,2,2,0,0,0,1,1,1]: this is also correct as it clusters points in the same way, but it just permuted the cluster names.  rand_score can handle this:"""

from sklearn import metrics

print(metrics.rand_score([0,0,0,1,1,1,2,2,2],[2,2,2,0,0,0,1,1,1]))

"""What value should K (n_clusters) be?  If we set it to different values and plot score or inertia, we often see an "elbow" in the curve.  But a better way is to find the silhouette score of the clustering for different K, and the best score tells us what K should be."""

from sklearn.metrics import silhouette_score

s = silhouette_score(X, km.labels_)
print(s)

"""Now instead of K-means let's use expectation maximisation (EM) for clustering, by fitting a Gaussian mixture model (GMM).  First generate a dataset:"""

from numpy import hstack
from numpy.random import normal
from matplotlib import pyplot

X1 = normal(loc=20, scale=5, size=3000)
X2 = normal(loc=40, scale=5, size=7000)
X = hstack((X1, X2))
X = X.reshape((len(X), 1))

"""A histogram shows the 2 clusters, which overlap."""

pyplot.hist(X, bins=50)
pyplot.show()

"""Now try to recreate the X1 and X2 data by fitting a GMM."""

from sklearn.mixture import GaussianMixture

gm = GaussianMixture(n_components=2)
gm.fit(X)

"""Now we can predict labels y for any points X.  To illustrate this, let's do it for the training data."""

y = gm.predict(X)
print(y[:100])
print(y[-100:])

"""We can also use a GMM *generatively*, to generate new data from the same distribution."""

Xnew, ynew = gm.sample(6)
print(Xnew)
print(ynew)

"""Let's also see DBSCAN in action.  Notice that we don't need to tell it K, but it does have other parameters such as eps(ilon) and min_samples."""

from sklearn.cluster import DBSCAN

X = np.array([[1,2],[2,2],[2,3],[8,7],[8,8],[25,80]])
db = DBSCAN(eps=3, min_samples=2)
clustering = db.fit(X)
print(clustering.labels_)

"""The last point (with label -1) is an outlier that doesn't seem to belong to any cluster."""





# -*- coding: utf-8 -*-
"""CS6319ARM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kTs6bCOan7tDJmM8LJgCQk5TzHQsEnAz

Use apyori.
"""

!pip install apyori

import numpy as np
import pandas as pd
from apyori import apriori

"""Read file."""

basket = pd.read_csv("https://github.com/andvise/DataAnalyticsDatasets/blob/78a9bd799b5fbf35344beff50304169f789d264c/Market_Basket_Optimisation.csv?raw=true",header=None)
print(basket)

"""Transform to a list of lists."""

records = []
for i in range(0, 7501):
    records.append([str(basket.values[i,j]) for j in range(0, 20)])

"""Apply Apriori."""

rules = apriori(records, min_support=0.0045, min_confidence=0.2, min_lift=3, min_length=2)
results = list(rules)

"""Count the rules."""

print(len(results))

"""View results."""

print(results[0])

"""All results."""

for item in results:
    pair = item[0]
    items = [x for x in pair]
    print("Rule: " + items[0] + " -> " + items[1])
    print("Support: " + str(item[1]))
    print("Confidence: " + str(item[2][0][2]))
    print("Lift: " + str(item[2][0][3]))
    print("=====================================")



# -*- coding: utf-8 -*-
"""CS6319DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rBuM7lYPEXGmCaNOc7_V9tjmxYiEiw7i

MLPRegressor experiment on Parkinsons data.
"""

import pandas as pd
import numpy as np
import sklearn
park = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data")
print(park)

"""Drop subject and motor_UPDRS columns."""

park.drop(labels=['subject#','motor_UPDRS'], axis=1, inplace=True)

"""Split into train and test."""

from sklearn.model_selection import train_test_split

parktrain, parktest = train_test_split(park, test_size=0.2)

"""Extract labels."""

trainlabs = parktrain.loc[:,'total_UPDRS']
traindata = parktrain.drop(labels=['total_UPDRS'], axis=1)
testlabs = parktest.loc[:,'total_UPDRS']
testdata = parktest.drop(labels=['total_UPDRS'], axis=1)

"""Rescale."""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
traindatar = scaler.fit_transform(traindata)
testdatar = scaler.transform(testdata)

"""Try a linear regressor on the training data with cross-validation."""

from sklearn.model_selection import cross_val_score
from sklearn.model_selection import ShuffleSplit
from sklearn.linear_model import LinearRegression

linreg = LinearRegression()
split = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(linreg, traindatar, trainlabs, scoring="neg_mean_squared_error", cv=split)
rmse = np.sqrt(-scores)
print("LS RMSE =",rmse.mean()," stddev =",scores.std())

"""And a random forest."""

from sklearn.ensemble import RandomForestRegressor

rfreg = RandomForestRegressor()
scores = cross_val_score(rfreg, traindatar, trainlabs, scoring="neg_mean_squared_error", cv=split)
rmse = np.sqrt(-scores)
print("RF RMSE =",rmse.mean()," stddev =",scores.std())

"""Better!  Now an MLP."""

from sklearn.neural_network import MLPRegressor

mlpreg = MLPRegressor(max_iter=10000)
scores = cross_val_score(mlpreg, traindatar, trainlabs, scoring="neg_mean_squared_error", cv=split)
rmse = np.sqrt(-scores)
print("MLP RMSE =",rmse.mean()," stddev =",scores.std())

"""Similar to RF.  Can we improve it?  """

mlpreg = MLPRegressor(hidden_layer_sizes=(30,20,10), max_iter=10000)
scores = cross_val_score(mlpreg, traindatar, trainlabs, scoring="neg_mean_squared_error", cv=split)
rmse = np.sqrt(-scores)
print("MLP RMSE =",rmse.mean()," stddev =",scores.std())

"""A bit better but not as good as RF.  Now try on the test data."""

from sklearn.metrics import mean_squared_error

linreg.fit(traindatar,trainlabs)
y = linreg.predict(testdatar)
mse = mean_squared_error(testlabs,y)
rmse = np.sqrt(mse)
print("LIN test error = ", rmse)

rfreg.fit(traindatar,trainlabs)
y = rfreg.predict(testdatar)
mse = mean_squared_error(testlabs,y)
rmse = np.sqrt(mse)
print("RF test error = ", rmse)

mlpreg.fit(traindatar,trainlabs)
y = mlpreg.predict(testdatar)
mse = mean_squared_error(testlabs,y)
rmse = np.sqrt(mse)
print("MLP test error = ", rmse)

"""Similar to the training data results: MLP similar to linear regressor, but RF much better.

Try MLPClassifier on the electrical grid stability dataset.  First the data wrangling as before.
"""

import pandas as pd
import numpy as np
import sklearn

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.neural_network import MLPClassifier

griddata = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv")

griddata = griddata.drop("stab", axis=1)
gridvars = griddata.drop("stabf", axis=1)
gridlabs = griddata["stabf"].copy()

scaler = MinMaxScaler()
gridvarsr = scaler.fit_transform(gridvars)

le = LabelEncoder()
gridlabse = le.fit_transform(gridlabs)

"""Now the classifier."""

classifier = MLPClassifier(hidden_layer_sizes=(200), max_iter=1000)
split = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(classifier, gridvarsr, gridlabse, scoring="accuracy", cv=split)
print("MLP accuracy =",scores.mean()," stddev =",scores.std())

"""Now let's see Keras and TensorFlow, using the grid classification problem.  Import libraries and wrangle the data as above, but this time I'll split it into train and test."""

import pandas as pd
import numpy as np
import sklearn
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

griddata = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv")

griddata = griddata.drop("stab", axis=1)
gridtrain, gridtest = train_test_split(griddata, test_size=0.2)

traindata = gridtrain.drop("stabf", axis=1)
trainlabs = gridtrain["stabf"].copy()
testdata = gridtest.drop("stabf", axis=1)
testlabs = gridtest["stabf"].copy()

scaler = MinMaxScaler()
traindatar = scaler.fit_transform(traindata)
testdatar = scaler.transform(testdata)

le = LabelEncoder()
trainlabse = le.fit_transform(trainlabs)
testlabse = le.transform(testlabs)

"""Now build an ANN sequentially in Keras."""

model = keras.models.Sequential()
model.add(keras.layers.Dense(200, activation="relu"))
model.add(keras.layers.Dense(1, activation="sigmoid"))
model.compile(loss="binary_crossentropy", metrics=["accuracy"])
model.fit(traindatar, trainlabse, epochs=50)
model.evaluate(testdatar, testlabse)

"""Now MNIST as an example of multiclass classification with Keras."""

import pandas as pd
import numpy as np
import sklearn
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import OneHotEncoder

mnisttrain = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tra", header=None)
mnisttest = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/optdigits/optdigits.tes", header=None)

traindata = mnisttrain.drop(64, axis=1)
trainlabs = mnisttrain[64].copy()
testdata = mnisttest.drop(64, axis=1)
testlabs = mnisttest[64].copy()
print(traindata)
enc = OneHotEncoder(sparse=False)
trainlabs = enc.fit_transform(trainlabs.to_numpy().reshape(-1, 1))
testlabs = enc.transform(testlabs.to_numpy().reshape(-1, 1))

"""Fit the network."""

model = keras.models.Sequential()
model.add(keras.layers.Dense(50, activation="relu", input_shape=(64,)))
model.add(keras.layers.Dense(10, activation="softmax"))
model.compile(loss="categorical_crossentropy", metrics=["accuracy"])
model.fit(traindata, trainlabs, epochs=50)

"""Finally, evaluate it on the test data."""

model.evaluate(testdata, testlabs)

"""Good result, but there seems to be a bit of overfitting.  Let's  use dropout.  I'll also add another hidden layer to show the use of dropout better."""

from keras.layers import Dropout

model = keras.models.Sequential()
model.add(Dropout(0.1, input_shape=(64,)))
model.add(keras.layers.Dense(50, activation="relu"))
model.add(Dropout(0.1))
model.add(keras.layers.Dense(50, activation="relu"))
model.add(keras.layers.Dense(10, activation="softmax"))
model.compile(loss="categorical_crossentropy", metrics=["accuracy"])
model.fit(traindata, trainlabs, epochs=50)

"""Evaluate again."""

model.evaluate(testdata, testlabs)

"""Nice: the train and test accuracies are about the same.

Now let's apply Keras to regression on the housing data.  For simplicity let's just use the numerical attributes (including the 3 new ones we created).
"""

import numpy as np
import pandas as pd
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras import layers
from keras.layers import Dropout

housing = pd.read_csv("https://github.com/ageron/handson-ml/raw/master/datasets/housing/housing.csv")

trainset, testset = train_test_split(housing, test_size=0.2)

traindata = trainset.drop("median_house_value", axis=1)
trainlabs = trainset["median_house_value"].copy()
trainnum = traindata.drop("ocean_proximity", axis=1)
testdata = testset.drop("median_house_value", axis=1)
testlabs = testset["median_house_value"].copy()
testnum = testdata.drop("ocean_proximity", axis=1)

trainnum["rooms_per_household"] = trainnum["total_rooms"] / trainnum["households"]
trainnum["bedrooms_per_room"] = trainnum["total_bedrooms"] / trainnum["total_rooms"]
trainnum["population_per_household"] = trainnum["population"] / trainnum["households"]
testnum["rooms_per_household"] = testnum["total_rooms"] / testnum["households"]
testnum["bedrooms_per_room"] = testnum["total_bedrooms"] / testnum["total_rooms"]
testnum["population_per_household"] = testnum["population"] / testnum["households"]

imputer = SimpleImputer(strategy="median")
imputer.fit(trainnum)
X = imputer.transform(trainnum)
trainnum = pd.DataFrame(X, columns=trainnum.columns)
Y = imputer.transform(testnum)
testnum = pd.DataFrame(Y, columns=testnum.columns)

scaler = StandardScaler()
trainnumr = scaler.fit_transform(trainnum)
testnumr = scaler.transform(testnum)

"""Fit a network to the training data."""

model = keras.models.Sequential()
model.add(keras.layers.Dense(100, activation="relu", input_shape=(11,)))
model.add(keras.layers.Dense(100, activation="relu"))
model.add(keras.layers.Dense(100, activation="relu"))
model.add(keras.layers.Dense(1))
model.compile(loss="mean_squared_error")
model.fit(trainnumr, trainlabs, epochs=300)

"""Evaluate on the test data."""

model.evaluate(testnumr, testlabs)

"""Pretty good: beats all other classifiers we tried, with little sign of overfitting even without dropout."""


# -*- coding: utf-8 -*-
"""CS6319Dimensionality.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12nXe1MU1fu5HC6Y5JfzM2igEUXCjxBb0

First a data file with a few hundred features.
"""

import pandas as pd
import numpy as np
import sklearn

secomdata = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data",
                        header=None,delim_whitespace=True)
secomlabs = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data",
                        header=None,delim_whitespace=True)
print(secomdata)
print(secomlabs)

"""For this experiment let's not split into train and test: we'll just apply cross-validation on the whole dataset.  First discard the 2nd column in the labels DataFrame, and encode the labels as they're in the form -1,+1."""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
secomlabs1 = secomlabs.iloc[:,0]
secomlabs2 = le.fit_transform(secomlabs1)

"""There seem to be missing values so impute the data."""

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median")
secomdata = imputer.fit_transform(secomdata)

"""The value ranges are different so rescale.  I'll use normalisation for no particular reason."""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
secomdata = scaler.fit_transform(secomdata)

"""For a change let's try a logistic regression classifier.  It fails to converge with the default number of iterations (100) so I increased it to 400."""

from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(max_iter=400)
split = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(classifier, secomdata, secomlabs2,
                        scoring="accuracy", cv=split)
print("LR acc mean =",scores.mean()," stddev =",scores.std())

"""Now let's try some feature selection methods.  First variance threshold.  Using 0.8 gave an error message (none met the threshold) but 0.9 reduces the number of variables to just 4."""

from sklearn.feature_selection import VarianceThreshold

sel = VarianceThreshold(threshold=(.9 * (1 - .9)))
secomsel = sel.fit_transform(secomdata)
print(secomsel)
split = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(classifier, secomsel, secomlabs2,
                        scoring="accuracy", cv=split)
print("LR acc mean =",scores.mean()," stddev =",scores.std())

"""No real difference in accuracy, and a faster runtime.  (It also works with the default max_iter=100.)  If we tested it on new data we might even find a better result, as overfitting is less likely  with fewer attributes.

Let's try again with a random forest, first on the reduced set of features.
"""

from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier()
scores = cross_val_score(classifier, secomsel, secomlabs2,
                        scoring="accuracy", cv=split)
print("RF acc mean =",scores.mean()," stddev =",scores.std())

"""Very similar result.  The random forest is slower.

Now let's try it on the full set of features.
"""

scores = cross_val_score(classifier, secomdata, secomlabs2,
                        scoring="accuracy", cv=split)
print("RF acc mean =",scores.mean()," stddev =",scores.std())

"""Similar accuracy but a much longer runtime.

Now let's try a higher-dimensional dataset.
"""

import numpy as np
import pandas as pd
import sklearn

arcenedata = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.data",
                        header=None, delim_whitespace=True)
arcenelabs = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/arcene/ARCENE/arcene_train.labels",
                        header=None, delim_whitespace=True)
print(arcenedata)
print(arcenelabs.value_counts())

"""Rescale data."""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
arcenedata = scaler.fit_transform(arcenedata)

"""Encode labels."""

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
arcenelabs2 = le.fit_transform(arcenelabs.values.ravel())

"""Select features using VarianceThreshold with 0.85 as 0.8 passed no features."""

from sklearn.feature_selection import VarianceThreshold

sel = VarianceThreshold(threshold=(.85 * (1 - .85)))
arcenesel = sel.fit_transform(arcenedata)
print(arcenesel.shape)

"""Classify on all features."""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import ShuffleSplit

split = ShuffleSplit(n_splits=10, test_size=0.2)
classifier = RandomForestClassifier()
scores = cross_val_score(classifier, arcenedata, arcenelabs2,
                        scoring="accuracy", cv=split)
print("acc mean =",scores.mean()," stddev =",scores.std())

"""And on the selected features."""

scores = cross_val_score(classifier, arcenesel, arcenelabs2,
                        scoring="accuracy", cv=split)
print("acc mean =",scores.mean()," stddev =",scores.std())

"""Try Chi2."""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

sel = SelectKBest(chi2, k=100)
arcenesel = sel.fit_transform(arcenedata, arcenelabs2)

"""And classify."""

scores = cross_val_score(classifier, arcenesel, arcenelabs2,
                        scoring="accuracy", cv=split)
print("acc mean =",scores.mean()," stddev =",scores.std())

"""Now try PCA."""

from sklearn.decomposition import PCA

pca = PCA(n_components=30)
arcenepca = pca.fit_transform(arcenedata)

"""And classify."""

scores = cross_val_score(classifier, arcenepca, arcenelabs2,
                        scoring="accuracy", cv=split)
print("acc mean =",scores.mean()," stddev =",scores.std())

"""Let's try a new PCA experiment using one of the datasets provided by sklearn.  We have to transform it to a DataFrame."""

from sklearn.datasets import load_breast_cancer

cancer0 = load_breast_cancer()
cancer = pd.DataFrame(data=cancer0.data, columns=cancer0.feature_names)

"""Take a look at the head of the data."""

cancer.head()

"""And its shape."""

cancer.shape

"""They need rescaling, and I'll use standardisation."""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X = scaler.fit_transform(cancer)

"""Now let's apply PCA.  We don't know how many dimensions to choose, so first let's choose all 30 (which is the default if we don't set n_components)."""

from sklearn.decomposition import PCA

pca30 = PCA()
X30 = pca30.fit_transform(X)

"""Using all features should explain all the variance.  Check this."""

print("explained variance: ",sum(pca30.explained_variance_ratio_))

"""Let's take a look at the explained_variance_ratio_ values using a cumulative sum."""

pca30.explained_variance_ratio_

"""They're listed biggest-first, and we see that the 1st component contains 44% of the variance, etc.  It's more helpful to see a cumulative sum."""

np.cumsum(pca30.explained_variance_ratio_)

"""Now let's see that as a graph."""

import matplotlib.pyplot as plt

plt.plot(np.cumsum(pca30.explained_variance_ratio_))

"""We can see that about 95% of the variance is explained by the first 10 components."""

np.cumsum(pca30.explained_variance_ratio_)[9]

"""Using the first 2 only explains about 63% but that's still useful for visualisation.  Let's plot the 2 components using cancer0.target as colour.  The 2 target values almost separate into 2 clusters."""

pca2 = PCA(n_components=2)
X2 = pca2.fit_transform(X)
plt.scatter(X2[:,0], X2[:,1], c=cancer0.target)



# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13aDenz2TxyxajhtGL8gdvU3ninhfI6UC
"""

import pandas as pd
from itertools import combinations

# a) Read the CSV file into a pandas DataFrame
# Assuming the file has no header and is space-separated
df = pd.read_csv("http://fimi.uantwerpen.be/data/chess.dat", header=None, sep=" ")

# b) Transform the DataFrame into a list of transactions
transactions = df.apply(lambda row: set(row.dropna().astype(str)), axis=1).tolist()

# c) Generate all association rules with length 2, support at least 0.01, confidence at least 2, and lift at least 3
min_support = 0.01
min_confidence = 2
min_lift = 3
rules = []
itemsets = set.union(*transactions)

for item1, item2 in combinations(itemsets, 2):
    support_item1 = sum(1 for transaction in transactions if item1 in transaction) / len(transactions)
    support_item2 = sum(1 for transaction in transactions if item2 in transaction) / len(transactions)
    support_itemset = sum(1 for transaction in transactions if {item1, item2}.issubset(transaction)) / len(transactions)

    print(f"Support for {item1}: {support_item1}")
    print(f"Support for {item2}: {support_item2}")
    print(f"Support for {item1} and {item2}: {support_itemset}")

    if support_itemset >= min_support:
        confidence_item1_to_item2 = support_itemset / support_item1
        confidence_item2_to_item1 = support_itemset / support_item2
        lift_item1_to_item2 = confidence_item1_to_item2 / support_item2
        lift_item2_to_item1 = confidence_item2_to_item1 / support_item1

        print(f"Confidence from {item1} to {item2}: {confidence_item1_to_item2}")
        print(f"Confidence from {item2} to {item1}: {confidence_item2_to_item1}")
        print(f"Lift from {item1} to {item2}: {lift_item1_to_item2}")
        print(f"Lift from {item2} to {item1}: {lift_item2_to_item1}")

        if (confidence_item1_to_item2 >= min_confidence and lift_item1_to_item2 >= min_lift):
            print(f"Rule added: {item1} -> {item2}")
            rules.append((item1, item2, support_itemset, confidence_item1_to_item2, lift_item1_to_item2))
        if (confidence_item2_to_item1 >= min_confidence and lift_item2_to_item1 >= min_lift):
            print(f"Rule added: {item2} -> {item1}")
            rules.append((item2, item1, support_itemset, confidence_item2_to_item1, lift_item2_to_item1))

# d) Print out the rules in a readable form
for rule in rules:
    antecedent = rule[0]
    consequent = rule[1]
    support = round(rule[2], 4)
    confidence = round(rule[3], 4)
    lift = round(rule[4], 4)
    print(f"Rule: {antecedent} -> {consequent} | Support: {support}, Confidence: {confidence}, Lift: {lift}")

# Print the number of rules found
print(f"Number of rules: {len(rules)}")

import pandas as pd
from itertools import combinations

# a) Read the CSV file into a pandas DataFrame
# Assuming the file has no header and is space-separated
df = pd.read_csv("http://fimi.uantwerpen.be/data/chess.dat", header=None, sep=" ")

# b) Transform the DataFrame into a list of transactions
transactions = df.apply(lambda row: set(row.dropna().astype(str)), axis=1).tolist()

# c) Generate all association rules with length 2, support at least 0.01
min_support = 0.01
rules = []
itemsets = set.union(*transactions)

for item1, item2 in combinations(itemsets, 2):
    support_item1 = sum(1 for transaction in transactions if item1 in transaction) / len(transactions)
    support_item2 = sum(1 for transaction in transactions if item2 in transaction) / len(transactions)
    support_itemset = sum(1 for transaction in transactions if {item1, item2}.issubset(transaction)) / len(transactions)

    print(f"Support for {item1}: {support_item1}")
    print(f"Support for {item2}: {support_item2}")
    print(f"Support for {item1} and {item2}: {support_itemset}")

    if support_itemset >= min_support:
        print(f"Rule added: {item1} -> {item2}")
        rules.append((item1, item2, support_itemset))

# d) Print out the rules in a readable form
for rule in rules:
    antecedent = rule[0]
    consequent = rule[1]
    support = round(rule[2], 4)
    print(f"Rule: {antecedent} -> {consequent} | Support: {support}")

# Print the number of rules found
print(f"Number of rules: {len(rules)}")

import pandas as pd
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# a) Read the CSV file into a pandas DataFrame
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00471/Data_for_UCI_named.csv"
df = pd.read_csv(url)

# b) Extract the 12 numerical attributes
numerical_attributes = df.select_dtypes(include=['number']).iloc[:, :12]

# c) Apply PCA to find the two main components
pca = PCA(n_components=2)
principal_components = pca.fit_transform(numerical_attributes)

# d) Plot the two components using a scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(principal_components[:, 0], principal_components[:, 1])
plt.title('PCA of 12 Numerical Attributes')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.grid(True)
plt.show()

# e) Find the total explained variance ratio of the two components
total_explained_variance_ratio = sum(pca.explained_variance_ratio_)
print("Total Explained Variance Ratio:", total_explained_variance_ratio)

# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KI6SnJfuf3YV3OmfxEXKpXpIYQYCH9te
"""

import numpy as np
from sklearn.mixture import GaussianMixture
from sklearn.metrics import adjusted_rand_score

# (a) Generate artificial 1-dimensional data
data1 = np.random.normal(loc=10, scale=3, size=100)
data2 = np.random.normal(loc=20, scale=2, size=100)
data = np.concatenate([data1, data2])

# (b) Fit a Gaussian mixture model
gmm = GaussianMixture(n_components=2, random_state=42)
gmm.fit(data.reshape(-1, 1))

# (c) Use the “predict” method to obtain predicted cluster labels
predicted_labels = gmm.predict(data.reshape(-1, 1))

# (d) Calculate the Rand score
true_labels = np.concatenate([np.zeros(100), np.ones(100)])  # Assuming 0 for the first 100 data points and 1 for the next 100
rand_score = adjusted_rand_score(true_labels, predicted_labels)
print("Adjusted Rand Score:", rand_score)

import pandas as pd
# Install apyori
!pip install apyori
from apyori import apriori

# (a) Read the CSV file into a pandas DataFrame
url = "http://fimi.uantwerpen.be/data/chess.dat"
df = pd.read_csv(url, header=None, sep=" ")

# (b) Transform the DataFrame into a form that the apyori system can use
transactions = df.apply(lambda row: row.dropna().astype(str).tolist(), axis=1).tolist()

# (c) Generate all association rules with length 2, support at least 0.01, confidence at least 2, and lift at least 3
rules = list(apriori(transactions, min_support=0.01, min_confidence=2, min_lift=3, min_length=2, max_length=2))
print(rules)
# (d) Print out the rules in a readable form
for rule in rules:
    antecedent = ', '.join(rule.items_base)
    consequent = ', '.join(rule.items_add)
    support = round(rule.support, 4)
    confidence = round(rule.ordered_statistics[0].confidence, 4)
    lift = round(rule.ordered_statistics[0].lift, 4)
    print(f"Rule: {antecedent} -> {consequent} | Support: {support}, Confidence: {confidence}, Lift: {lift}")


# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OsrmBsSd6t4DEiYitNDXGIrks6eTsx34
"""

import numpy as np

# Generate data with mean 10 and standard deviation 3
data1 = np.random.normal(loc=10, scale=3, size=100)

# Generate data with mean 20 and standard deviation 2
data2 = np.random.normal(loc=20, scale=2, size=100)

# Combine the two datasets
data = np.concatenate([data1, data2])

# Shuffle the data
np.random.shuffle(data)

# Print the first 10 elements to verify
print(data[:10])

from sklearn.mixture import GaussianMixture

# Create the Gaussian Mixture Model
gmm = GaussianMixture(n_components=2, random_state=42)

# Fit the model to the data
gmm.fit(data.reshape(-1, 1))

# Predict the cluster labels for the data
labels = gmm.predict(data.reshape(-1, 1))

# Print the first 10 labels to verify
print(labels[:10])

from sklearn.metrics import adjusted_rand_score

# Generate true labels based on means
true_labels = np.concatenate([np.zeros(100), np.ones(100)])

# Calculate the adjusted Rand score
rand_score = adjusted_rand_score(true_labels, labels)

# Print the Rand score
print("Adjusted Rand Score:", rand_score)


# -*- coding: utf-8 -*-
"""FS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oA2BZ7-91_bfcVism15qPl8b9LGfvwD9

First, we need to install the tsfresh module using pip
"""

!pip install tsfresh

"""we need to download sample data from the UCI Machine Learning Repository that we can use for our experimentations.

The documentation for the dataset is provided on: http://archive.ics.uci.edu/ml/datasets/Robot+Execution+Failures

The dataset represents Force and Torque measurements from sensors on robots. The dataset contains 88 samples represented by the ‘id’ column. The time column represents the sequence of readings.
"""

from tsfresh.examples.robot_execution_failures import download_robot_execution_failures,load_robot_execution_failures

download_robot_execution_failures()
df, y = load_robot_execution_failures()

df

"""in the dataframe we have 1320 rows and 8 columns"""

y

"""The ‘y’ column represents whether or not the sensonrs data represents robot failure. This is a target value, and our goal can be to classify torque and force measurements as either a failure or not.

we need to import the ‘extract_features’ method and use it to extract features for our dataset. We need to pass the data to the ‘extract_features’ method along with information on which column represents the sequence of readings and the ‘id’ column that will be used to differentiate between various datasets. In our case, readings from every robot represent one dataset, differentiated by the ‘id’ column and sorted on the ‘time’ column.
"""

from tsfresh import extract_features

features = extract_features(df, column_id="id", column_sort="time")
features

"""we can see that the tsfresh package has returned 4698 columns that include time-series features for all the datasets and all the numeric columns in our datasets.

You can use the following code to identify all the features calculated by the tsfresh
"""

import tsfresh.feature_extraction.settings

kind_to_fc_parameters_all = tsfresh.feature_extraction.settings.from_columns(features)
kind_to_fc_parameters_all

"""after automated feature engineering, we pass to feature selection, firstly via a filter-based method"""

import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing
from sklearn.feature_selection import (
    f_regression,
    SelectPercentile,
)

import numpy as np
features = features.replace([np.nan, -np.inf], 0)
#features = features.drop('index', axis=1)

features = features.applymap(lambda x: min(x, 999))

features = features.applymap(lambda x: max(x, -999))

features

"""f_regression is therefore recommended as a feature selection criterion to identify potentially predictive feature for a downstream classifier, irrespective of the sign of the association with the target variable.

f_regression is derived from r_regression and will rank features in the same order if all the features are positively correlated with the target.
"""

univariate = f_regression(features, y)

univariate

"""plot the correlation values for each feature vs the target"""

plt.rc("axes", titlesize=15) #fontsize of the title
univariate = pd.Series(univariate[1])
univariate.index = features.columns
univariate.sort_values(ascending=True).plot.bar(figsize=(10, 5), rot=45)
plt.ylabel("p-values")
plt.title("Correlation")
plt.show()

"""Select features according to a percentile of the highest scores"""

sel = SelectPercentile(f_regression, percentile=30).fit(features, y)
sel.get_feature_names_out()

features_t = sel.transform(features)
features_t = pd.DataFrame(features_t, columns=sel.get_feature_names_out())

features_t

"""based on this preliminary filtering step, the number of features has been reduced to 1410

however, there is still the problem of multi-collinearity among those features

The script below removes 50 features at the time using VIF, just to speed things up. Ideally you should be doing this one step at the time
"""

import os
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer

#Applying multicollinearity to remove columns which are dependent on each other
# From looking at documentation, values between 5 and 10 are "okay".
# Above 10 is too high and so should be removed.
from statsmodels.stats.outliers_influence import variance_inflation_factor

class ReduceVIF(BaseEstimator, TransformerMixin):
    def __init__(self, thresh=50, impute=True, impute_strategy='median'):
        # From looking at documentation, values between 5 and 10 are "okay".
        # Above 10 is too high and so should be removed.
        self.thresh = thresh

        # The statsmodel function will fail with NaN values, as such we have to impute them.
        # By default we impute using the median value.
        # This imputation could be taken out and added as part of an sklearn Pipeline.
        if impute:
            self.imputer = SimpleImputer(strategy=impute_strategy)

    def fit(self, X, y=None):
        print('ReduceVIF fit')
        if hasattr(self, 'imputer'):
            self.imputer.fit(X)
        return self

    def transform(self, X, y=None):
        print('ReduceVIF transform')
        columns = X.columns.tolist()
        if hasattr(self, 'imputer'):
            X = pd.DataFrame(self.imputer.transform(X), columns=columns)
        return ReduceVIF.calculate_vif(X, self.thresh)

    @staticmethod
    def calculate_vif(X, thresh=50.0, max_drop=50):
      dropped = True
      while dropped and len(X.columns) > max_drop:
        variables = X.columns
        dropped = False
        vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]

        max_vif_indices = sorted(range(len(vif)), key=lambda i: vif[i], reverse=True)[:max_drop]
        max_vif_values = [vif[i] for i in max_vif_indices]

        if max(max_vif_values) > thresh:
            for i in max_vif_indices:
                print(f'Dropping {X.columns[i]} with vif={vif[i]}')
            X = X.drop(X.columns[max_vif_indices], axis=1)
            dropped = True
      return X

Mult_Coll = ReduceVIF()
features_t2 = Mult_Coll.fit_transform(features_t)
features_t2.head()

"""only 10 features have been left based on the previous step.
The lack of multi-collinearity can be visualized using the correlation matrix
"""

import seaborn as sns

sns.heatmap(features_t2.corr().abs(),annot=True)

"""We can reduce the number of features even more via dimensionality reduction (for instance via PCA).
In this particular case we only preserve a number of principal components which covers 95% of the explained variance.
Remember that each PC is a linear combination of the 10 variable left at the previous step
"""

from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)
features_t3 = pca.fit_transform(features_t2)

print(pca.explained_variance_ratio_)

print(pca.singular_values_)

features_t3

features_t3 = pd.DataFrame(features_t3, columns=['PC1', 'PC2', 'PC3'])
print(features_t3)

"""Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.


"""

from sklearn.feature_selection import RFE
from sklearn import tree

clf = tree.DecisionTreeClassifier()
selector = RFE(clf, n_features_to_select=2, step=1)
selector2 = selector.fit(features_t3, y)
selector = selector.fit_transform(features_t3, y)

selector2.support_

selector2.ranking_

features_t4 = pd.DataFrame(selector, columns=['First', 'Second'])
print(features_t4)

"""let's train the resulting featutre set on a decision tree classifier, and then test it (just as an idea we also evaluate the claasifier on the same dataset even though it is not the right thing to do)"""

clf = tree.DecisionTreeClassifier()
clf = clf.fit(features_t4, y)

tree.plot_tree(clf)

predictions = clf.predict(features_t4)

from sklearn.metrics import accuracy_score
score = accuracy_score(y, predictions)
score




# -*- coding: utf-8 -*-
"""Lab4 (3) (3).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14f8nPfacq9_EkU3uFvgVgjloJWvNZjDY

Name - Shubham Bakshi
Student Number - 123104613
"""

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import homogeneity_score
from sklearn import metrics

# Read the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt"
columns = ["area", "perimeter", "compactness", "length_of_kernel", "width_of_kernel", "asymmetry_coefficient", "length_of_kernel_groove", "seed_type"]
seed_dataset = pd.read_csv(url, sep="\s+", names=columns)
print(seed_dataset.describe())

sd_data = seed_dataset.drop("seed_type", axis=1)

# Data Standardisation
scaler = StandardScaler()
sd_data_scaled = scaler.fit_transform(sd_data)

# Apply K-Means Clustering
kmeans = KMeans(n_clusters=3, n_init=20,random_state=42)
kmeans_preds = kmeans.fit_predict(sd_data_scaled)

# Apply EM (Expectation-Maximization) clustering
em = GaussianMixture(n_components=3, random_state=42)
em.fit(sd_data_scaled)
em_preds = em.predict(sd_data_scaled)

# Print out the predictions
print("K-means predictions:")
print(kmeans_preds)
print("\nEM predictions:")
print(em_preds)

# Compute the Rand Index scores
actual_labels = seed_dataset["seed_type"]
kmeans_rand_score = metrics.rand_score(actual_labels, kmeans_preds)
em_rand_score = metrics.rand_score(actual_labels, em_preds)

# Print out the Rand Index scores
print("Rand Index Scores:")
print("K-means:", kmeans_rand_score)
print("EM:", em_rand_score)


# -*- coding: utf-8 -*-
"""Lab5_123104613 (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZfcDq59SsXmkq40S1MrtO5mzCul4GkCC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

# Load the Parkinson dataset
parkinson_data_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/telemonitoring/parkinsons_updrs.data"
parkinson_data = pd.read_csv(parkinson_data_url)
print(parkinson_data.describe())

# Drop columns not needed for features and labels
features = parkinson_data.drop(columns=['subject#', 'motor_UPDRS', 'total_UPDRS'])
labels = parkinson_data['total_UPDRS']

print("Features Shape:", features.shape)
print("Labels Shape:", labels.shape)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=49)

print("Training Features Shape:", X_train.shape)
print("Training Labels Shape:", y_train.shape)
print("Testing Features Shape:", X_test.shape)
print("Testing Labels Shape:", y_test.shape)

# Build the neural network model
model = Sequential()

# Add input layer
model.add(Dense(256, activation='sigmoid', input_shape=(X_train.shape[1],)))

# Define hyperparameters for hidden layers
hidden_layers = [
    (512, 0.5),
    (512, 0.5),
    (512, 0.5),
    (512, 0.5),
    (256, 0.5),
    (256, 0.5),
    (256, 0.5),
    (128, 0.5),
    (128, 0.5),
    (128, 0.5),
    (128, 0.5),
    (128, 0.5),
    (64, 0.5),
    (64, 0.5),
    (64, 0.5),
    (64, 0.5)
]

# Add hidden layers
for units, dropout_rate in hidden_layers:
    model.add(Dense(units, activation='relu'))
    model.add(Dropout(dropout_rate))

# Add output layer
model.add(Dense(1, activation='linear'))

# Compile the model
optimizer = Adam(learning_rate=0.0001)
model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])

# Define variables for epochs and batch size
epochs = 100
batch_size = 64

# Train the model
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, verbose=1)

# Evaluate the model on the test set
print("Evaluating the model on the test set...")
loss, mae = model.evaluate(X_test, y_test, verbose=0)
print("Mean Absolute Error on Test Set:", mae)
print("Loss on Test Set:", loss)

# Plot training history
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()



# -*- coding: utf-8 -*-
"""ML_A2_Shubham3 (4) (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11-6CbQEy2fiWwp3Wi0LVTHdYe8zRqvqf
"""

"""
Assignment 3
CS6319: Applied Machine Learning
Name: Shubham Bakshi
Student ID: 123104613
"""

#Import Libraries
import pandas as pd
import os
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, ShuffleSplit
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt

# Download the dataset if not already available locally
if not os.path.exists("data_banknote_authentication.txt"):
    url = "https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt"
    dataset = pd.read_csv(url, header=None)
else:
    dataset = pd.read_csv("data_banknote_authentication.txt", header=None)

# Separate x and y i.e. all the features and test variable
X = dataset.iloc[:, :-1]
Y = dataset.iloc[:, -1]  # Last column

# Splitting dataset into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=10)

# Printing the first 5 rows
print(dataset.head())

# Printing the shape of the dataset
print(dataset.shape)

# Printing some basic statistics
print(dataset.describe())

def evaluate_classifier(classifier):

    # Fit classifier with train and test
    classifier.fit(X_train, Y_train)
    # Predict the target data
    Y_pred = classifier.predict(X_test)

    # Check Accuracy & scores
    accuracy = accuracy_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)
    roc_auc = roc_auc_score(Y_test, classifier.predict_proba(X_test)[:, 1])

    # Find the best classifier on the available test data
    f1_test = f1_score(Y_test, Y_pred)

    return {
        "accuracy": accuracy,
        "precision": precision,
        "recall": recall,
        "f1_test":f1_test,
        "roc_auc": roc_auc
        }

def evaluate_classifier_train(cf,X_train, X_test, Y_train, Y_test):
    cf.fit(X_train, Y_train)
    Y_pred = cf.predict(X_test)

    # Check Accuracy & scores
    accuracy = accuracy_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)
    roc_auc = roc_auc_score(Y_test, cf.predict_proba(X_test)[:, 1])
    return accuracy, precision, recall, roc_auc

classifiers = {
    "Random Forest":     RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "Support Vector Machine": SVC(probability=True)
}

results = {}

# shuffle split
shuffle_split = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)
# Initialize evaluation metrics
cv_accuracy = []
cv_precision = []
cv_recall = []
cv_roc_auc = []
for name, classifier in classifiers.items():
    #evaluate on train_data
    accuracy, precision, recall,roc_auc = evaluate_classifier_train(classifier,X_train, X_test, Y_train, Y_test)
    # Store evaluation metrics
    cv_accuracy.append(accuracy)
    cv_precision.append(precision)
    cv_recall.append(recall)
    cv_roc_auc.append(roc_auc)
    # Print average evaluation metrics for each fold
    print("Classier Name:",classifier)
    average_accuracy = np.mean(cv_accuracy)
    print("Average Accuracy:", "{:.4f}".format(average_accuracy))
    average_precision = np.mean(cv_accuracy)
    print("Average Precision:", "{:.4f}".format(average_precision))
    average_recall = np.mean(cv_recall)
    print("Average Recall:", "{:.4f}".format(average_recall))
    average_roc_auc = np.mean(cv_roc_auc)
    print("Average ROC AUC:", "{:.4f}".format(average_roc_auc))

for name, classifier in classifiers.items():
    # scores
    results[name] = cross_val_score(classifier, X_train, Y_train, cv = shuffle_split, scoring='f1').mean()

# results
print("{:<35} | {:<40}".format('Classifier','Score') )
print("-" * len("{:<35} {:<40}".format('Classifier','Score')))
for key, value in results.items():
    print("{:35} | {:<40}".format(key, value))

# Choose classifier with best F1 score
best_classifier = max( results, key=results.get )
score = evaluate_classifier( classifiers[best_classifier])

print("Best classifier {} and mean {}\n".format(best_classifier, results[best_classifier]))
print("F1 score on test dataset is {}\n".format(score['f1_test']))

# Print the results
print("All the Scores using test data for", best_classifier)
print("{:<35} | {:<40}".format('Parameter','Score') )
print("-" * len("{:<35} {:<40}".format('Parameter','Score')))
for key, value in score.items():
    print("{:35} | {:<40}".format(key, value))

# new code to manipulate the results

scores = {}
cs_values = [0.1,0.15, 0.2, 0.25, 0.3, 0.4, 0.5 , .7]

# will iterate over these values
for c in cs_values:
    classifier = SVC(C=c)
    classifier.fit(X_train, Y_train)
    Y_pred = classifier.predict(X_test)
    scores[c] = f1_score(Y_test,Y_pred)

print(scores)

# new code to manipulate the results

scores2 = {}
gammas = [0.1, 1, 10, 100]

# will iterate over these values
for g in gammas:
    classifier2 = SVC(gamma=g)
    classifier2.fit(X_train, Y_train)
    Y_pred = classifier2.predict(X_test)
    scores2[g] = f1_score(Y_test,Y_pred)

print(scores2)

# Plotting graphs for this values,

import matplotlib.pyplot as plt
# Extracting k and scores
x = list(scores.keys())
y = list(scores.values())
# X axis parameter:
xaxis = np.array(x)
# Y axis parameter:
yaxis = np.array(y)
plt.xlabel('K Values')
plt.ylabel('F1 Scores')
plt.title('K Values and F1 Scores')
plt.plot(xaxis, yaxis)
plt.show()
#With C(penalty parameter of the error term) value 0.4 we get the best results then F1 Score is constant for other values

# Plotting graphs for this values,
import matplotlib.pyplot as plt
# Extracting k and scores
x = list(scores2.keys())
y = list(scores2.values())
# X axis parameter:
xaxis = np.array(x)
# Y axis parameter:
yaxis = np.array(y)
plt.xlabel('Gamma Values')
plt.ylabel('F1 Scores')
plt.title('Gamma Values and F1 Scores')
plt.plot(xaxis, yaxis)
plt.show()
#With Gamma (higher the gamma value it tries to exactly fit the training data set)  we get the best results when gamma is low at very high gamma F1 Score decrease


